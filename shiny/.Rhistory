paste("Accuracy =",round(perf[2]/(perf[1]+perf[2]),digits = 2))
paste("Unknown words =",sum(triFault))
head(termFreqTableT[which(predResults!=termFreqTableT$s3),])
head(predResults[which(predResults!=termFreqTableT$s3)])
head(termFreqTableT[which(triFault>0),])
testResults<-cbind(termFreqTableT,predResults)
termFreqTableT <- read.csv("model-data/termFreqTableT.csv",stringsAsFactors=FALSE,header=TRUE)
l<-str_extract_all(termFreqTableT$term,"[^[:blank:]]+")
temp<-as.data.frame(do.call(rbind,l),stringsAsFactors = FALSE)
names(temp)<-c("s1","s2","s3")
termFreqTableT<-cbind(termFreqTableT,temp)
remove(temp)
termFreqTableT<-termFreqTableT[termFreqTableT$freq>10,]
predResults<-vector(mode = "character",length = nrow(termFreqTableT))
triFault<-vector(length=nrow(termFreqTableT))
for(i in 1:nrow(termFreqTableT)) {
matches<-tri_next_word(termFreqTable3,termFreqTableT[i,]$s1,termFreqTableT[i,]$s2)
triFault[i]<-ifelse(is.na(matches[1]),1,0)
if(is.na(matches[1])) matches<-bi_next_word(termFreqTable2,termFreqTableT[i,]$s2)
predResults[i]<-matches[1]
}
perf<-table(predResults==termFreqTableT$s3)
paste("Accuracy =",round(perf[2]/(perf[1]+perf[2]),digits = 2))
paste("Unknown words =",sum(triFault))
head(termFreqTableT[which(predResults!=termFreqTableT$s3),])
head(predResults[which(predResults!=termFreqTableT$s3)])
head(termFreqTableT[which(triFault>0),])
testResults<-cbind(termFreqTableT,predResults)
termFreqTableT <- read.csv("model-data/termFreqTableT_l.csv",stringsAsFactors=FALSE,header=TRUE)
l<-str_extract_all(termFreqTableT$term,"[^[:blank:]]+")
temp<-as.data.frame(do.call(rbind,l),stringsAsFactors = FALSE)
names(temp)<-c("s1","s2","s3")
termFreqTableT<-cbind(termFreqTableT,temp)
remove(temp)
termFreqTableT<-termFreqTableT[termFreqTableT$freq>10,]
predResults<-vector(mode = "character",length = nrow(termFreqTableT))
triFault<-vector(length=nrow(termFreqTableT))
for(i in 1:nrow(termFreqTableT)) {
matches<-tri_next_word(termFreqTable3,termFreqTableT[i,]$s1,termFreqTableT[i,]$s2)
triFault[i]<-ifelse(is.na(matches[1]),1,0)
if(is.na(matches[1])) matches<-bi_next_word(termFreqTable2,termFreqTableT[i,]$s2)
predResults[i]<-matches[1]
}
perf<-table(predResults==termFreqTableT$s3)
paste("Accuracy =",round(perf[2]/(perf[1]+perf[2]),digits = 2))
paste("Unknown words =",sum(triFault))
head(termFreqTableT[which(predResults!=termFreqTableT$s3),])
head(predResults[which(predResults!=termFreqTableT$s3)])
head(termFreqTableT[which(triFault>0),])
testResults<-cbind(termFreqTableT,predResults)
source('~/Projects/Capstone/textprediction/gen-model-data-v1.R', echo=TRUE)
termFreqTable$prob <- termFreqTable$freq/sum(termFreqTable$freq)
hist(termFreqTable$prob)
head(termFreqTable)
sum(termFreqTable$prob)
termFreqTable2$prob <- termFreqTable2$freq/sum(termFreqTable2$freq)
termFreqTable3$prob <- termFreqTable3$freq/sum(termFreqTable3$freq)
termFreqTable4$prob <- termFreqTable4$freq/sum(termFreqTable4$freq)
head(termFreqTable4)
termFreqTable3[grep("for the follow",termFreqTable3$term),]
log(0.018)-log(0.0019)
source('~/Projects/Capstone/textprediction/gen-model-data-v1.R', echo=TRUE)
View(termFreqTable4)
View(termFreqTable)
v<-NGramTokenizer(textASCII, Weka_control(min = 4, max = 4,delimiters=" \r\n\t.,;:\"()?!"))
termFreqTableT<-as.data.frame(table(v),stringsAsFactors=FALSE)
names(termFreqTableT)<-c("term","freq")
termFreqTableT<-termFreqTableT[termFreqTableT$freq>2,]
termFreqTableT<-termFreqTableT[order(-termFreqTableT$freq),]
termFreqTableT$prob <- termFreqTableT$freq/sum(termFreqTableT$freq)
# write final test set to file
write.csv(termFreqTableT,"termFreqTableT.csv", row.names = FALSE)
View(termFreqTableT)
0.3/0.2
0.3/0.1
0.3*0.2
0.3*0.1
setwd(~)
setwd()
setwd("~")
setwd("~/Projects")
require("RWeka")
#require("gsubfn")
require("ggplot2")
require("plyr")
require("qdap")
require("stringr")
require("slam")
setwd("~/Projects/Capstone/textprediction")
#set size tag for output files
sizeLabel<-"_unigram_prob_used_10-4"
news <- readLines("../data/en_US.news.txt",n=-1,encoding="ISO-8859-1")
blogs<- readLines("../data/en_US.blogs.txt",n=-1,encoding="ISO-8859-1")
twitter<- readLines("../data/en_US.twitter.txt",n=-1,encoding="ISO-8859-1")
Text<-c(news,blogs,twitter)
#### sammple x elements to reduce  size of data set ####
set.seed(39955)
#red<-seq(1,length(Text),by=10)
text<-sample(Text,50000)
#### clean imported text and create corpus  #####################################################
# find non-ascii chars and remove them
text_<-iconv(text, "ISO-8859-1", "ASCII", sub="byte")
textASCII<-gsub("<[0-9|a-z][0-9|a-z]>","",text_)
#test
#grep("<[0-9|a-z][0-9|a-z]>",text_)
#grep("<[0-9|a-z][0-9|a-z]>",textASCII)
# make some space
remove(news,blogs,twitter,text,text_)
#remove(text,text_)
# remove quotes and forward slashes (not covered by removePunctuation)
textASCII<-gsub("\"","",textASCII)
textASCII<-gsub("/","",textASCII)
#more tests
#grep("\"",textASCII)
#grep("/",textASCII)
#replace sequences of "-" with whitespace
textASCII<-gsub("-+"," ",textASCII)
#remove extra whitespace, quotes, spaces before commas
#textASCII <- scrubber(textASCII)
#strip digits and force all chars to lower case
textASCII <- strip(textASCII,char.keep="'",apostrophe.remove=FALSE)
#### Tokenize the Corpus ######################################################
v<-NGramTokenizer(textASCII, Weka_control(min = 1, max = 1,delimiters=" \r\n\t.,;:\"()?!"))
termFreqTable<-as.data.frame(table(v),stringsAsFactors=FALSE)
names(termFreqTable)<-c("term","freq")
termFreqTable<-termFreqTable[termFreqTable$freq>4,]
termFreqTable<-termFreqTable[order(-termFreqTable$freq),]
v<-NGramTokenizer(textASCII, Weka_control(min = 2, max = 2,delimiters=" \r\n\t.,;:\"()?!"))
termFreqTable2<-as.data.frame(table(v),stringsAsFactors=FALSE)
names(termFreqTable2)<-c("term","freq")
termFreqTable2<-termFreqTable2[termFreqTable2$freq>4,]
termFreqTable2<-termFreqTable2[order(-termFreqTable2$freq),]
v<-NGramTokenizer(textASCII, Weka_control(min = 3, max = 3,delimiters=" \r\n\t.,;:\"()?!"))
termFreqTable3<-as.data.frame(table(v),stringsAsFactors=FALSE)
names(termFreqTable3)<-c("term","freq")
termFreqTable3<-termFreqTable3[termFreqTable3$freq>4,]
termFreqTable3<-termFreqTable3[order(-termFreqTable3$freq),]
v<-NGramTokenizer(textASCII, Weka_control(min = 4, max = 4,delimiters=" \r\n\t.,;:\"()?!"))
termFreqTable4<-as.data.frame(table(v),stringsAsFactors=FALSE)
names(termFreqTable4)<-c("term","freq")
termFreqTable4<-termFreqTable4[termFreqTable4$freq>4,]
termFreqTable4<-termFreqTable4[order(-termFreqTable4$freq),]
#v<-NGramTokenizer(textASCII, Weka_control(min = 1, max = 3,delimiters=" \r\n\t.,;:\"()?!"))
#termFreqTableN<-as.data.frame(table(v),stringsAsFactors=FALSE)
#names(termFreqTableN)<-c("term","freq")
#termFreqTableN<-termFreqTableN[termFreqTableN$freq>1,]
#termFreqTableN<-termFreqTableN[order(-termFreqTableN$freq),]
### Explore x-grams ##########################################################
stats<-function(freq) {
print(paste("Num = ",length(freq)))
print(paste("Mean = ",round(mean(freq),1)))
print(paste("Standard Dev = ",round(sd(freq),1)))
print(paste("max frequency = ",max(freq)))
print(paste("min frequency = ",min(freq)))
hist(log(freq))
rug(jitter(log(freq),amount=0.1),col="red")
boxplot(freq)
}
# get distribution stats
stats(termFreqTable$freq)
stats(termFreqTable2$freq)
stats(termFreqTable3$freq)
stats(termFreqTable4$freq)
# review most frequent x-grams
print("Most frequent 1-grams:")
head(termFreqTable[order(-termFreqTable$freq),])
print("Most frequent bigrams:")
head(termFreqTable2[order(-termFreqTable2$freq),])
print("Most frequent trigrams:")
head(termFreqTable3[order(-termFreqTable3$freq),])
print("Most frequent 4-grams:")
head(termFreqTable4[order(-termFreqTable4$freq),])
#decompose ngrams to facilitate prob calcs
l<-str_extract_all(termFreqTable2$term,"[^[:blank:]]+")
temp<-as.data.frame(do.call(rbind,l),stringsAsFactors = FALSE)
names(temp)<-c("s1","s2")
termFreqTable2<-cbind(termFreqTable2,temp)
remove(temp)
l<-str_extract_all(termFreqTable3$term,"[^[:blank:]]+")
temp<-as.data.frame(do.call(rbind,l),stringsAsFactors = FALSE)
names(temp)<-c("s1","s2","s3")
termFreqTable3<-cbind(termFreqTable3,temp)
remove(temp)
l<-str_extract_all(termFreqTable4$term,"[^[:blank:]]+")
temp<-as.data.frame(do.call(rbind,l),stringsAsFactors = FALSE)
names(temp)<-c("s1","s2","s3","s4")
termFreqTable4<-cbind(termFreqTable4,temp)
remove(temp)
# calc n-gram probs from frequencies
# first unigrams
termFreqTable$prob <- termFreqTable$freq/sum(termFreqTable$freq)
# then bigrams
uni<-vector(mode="numeric",length=nrow(termFreqTable2))
for(i in 1:nrow(termFreqTable2)) {
uni[i] <- termFreqTable[termFreqTable$term==termFreqTable2[i,]$s2,]$freq
}
termFreqTable2$prob <- termFreqTable2$freq/uni
# then trigrams
uni<-vector(mode="numeric",length=nrow(termFreqTable3))
for(i in 1:nrow(termFreqTable3)) {
uni[i] <- termFreqTable[termFreqTable$term==termFreqTable3[i,]$s3,]$freq
}
termFreqTable3$prob <- termFreqTable3$freq/uni
#then quadgrams
uni<-vector(mode="numeric",length=nrow(termFreqTable4))
for(i in 1:nrow(termFreqTable4)) {
uni[i] <- termFreqTable[termFreqTable$term==termFreqTable4[i,]$s4,]$freq
}
termFreqTable4$prob <- termFreqTable4$freq/uni
## persist ngram models to csv files ###############################################
write.csv(termFreqTable,paste("termFreqTable",sizeLabel,".csv",sep=""), row.names = FALSE)
write.csv(termFreqTable2,paste("termFreqTable2",sizeLabel,".csv",sep=""), row.names = FALSE)
write.csv(termFreqTable3,paste("termFreqTable3",sizeLabel,".csv",sep=""), row.names = FALSE)
write.csv(termFreqTable4,paste("termFreqTable4",sizeLabel,".csv",sep=""), row.names = FALSE)
news <- readLines("../data/en_US.news.txt",n=-1,encoding="ISO-8859-1")
blogs<- readLines("../data/en_US.blogs.txt",n=-1,encoding="ISO-8859-1")
twitter<- readLines("../data/en_US.twitter.txt",n=-1,encoding="ISO-8859-1")
text<-c(news,blogs,twitter)
#### sammple different set of elements for testing ####
#### sammple x elements to reduce  size of data set ####
set.seed(39900)
#red<-seq(1,length(Text),by=10)
text<-sample(Text,50000)
#### clean imported text and create corpus  #####
# find non-ascii chars and remove them
text_<-iconv(text, "ISO-8859-1", "ASCII", sub="byte")
textASCII<-gsub("<[0-9|a-z][0-9|a-z]>","",text_)
#test
#grep("<[0-9|a-z][0-9|a-z]>",text_)
#grep("<[0-9|a-z][0-9|a-z]>",textASCII)
# make some space
remove(news,blogs,twitter,text,text_)
# remove quotes and forward slashes (not covered by removePunctuation)
textASCII<-gsub("\"","",textASCII)
textASCII<-gsub("/","",textASCII)
#more tests
#grep("\"",textASCII)
#grep("/",textASCII)
#replace sequences of "-" with whitespace
textASCII<-gsub("-+"," ",textASCII)
#remove extra whitespace, quotes, spaces before commas
#textASCII <- scrubber(textASCII)
#strip digits and force all chars to lower case
textASCII <- strip(textASCII,char.keep="'",apostrophe.remove=FALSE)
#### Tokenize the Corpus
# for final test set reduce frequency threshold for keepers
v<-NGramTokenizer(textASCII, Weka_control(min = 4, max = 4,delimiters=" \r\n\t.,;:\"()?!"))
termFreqTableT<-as.data.frame(table(v),stringsAsFactors=FALSE)
names(termFreqTableT)<-c("term","freq")
termFreqTableT<-termFreqTableT[termFreqTableT$freq>2,]
termFreqTableT<-termFreqTableT[order(-termFreqTableT$freq),]
l<-str_extract_all(termFreqTableT$term,"[^[:blank:]]+")
temp<-as.data.frame(do.call(rbind,l),stringsAsFactors = FALSE)
names(temp)<-c("s1","s2","s3","s4")
termFreqTableT<-cbind(termFreqTableT,temp)
remove(temp)
# write final test set to file
write.csv(termFreqTableT,paste("termFreqTableT",sizeLabel,".csv",sep=""), row.names = FALSE)
news <- readLines("../data/en_US.news.txt",n=-1,encoding="ISO-8859-1")
blogs<- readLines("../data/en_US.blogs.txt",n=-1,encoding="ISO-8859-1")
twitter<- readLines("../data/en_US.twitter.txt",n=-1,encoding="ISO-8859-1")
text<-c(news,blogs,twitter)
#### sammple different set of elements for testing ####
#### sammple x elements to reduce  size of data set ####
set.seed(39900)
#red<-seq(1,length(Text),by=10)
text<-sample(Text,50000)
#### clean imported text and create corpus  #####
# find non-ascii chars and remove them
text_<-iconv(text, "ISO-8859-1", "ASCII", sub="byte")
textASCII<-gsub("<[0-9|a-z][0-9|a-z]>","",text_)
#test
#grep("<[0-9|a-z][0-9|a-z]>",text_)
#grep("<[0-9|a-z][0-9|a-z]>",textASCII)
# make some space
remove(news,blogs,twitter,text,text_)
# remove quotes and forward slashes (not covered by removePunctuation)
textASCII<-gsub("\"","",textASCII)
textASCII<-gsub("/","",textASCII)
#more tests
#grep("\"",textASCII)
#grep("/",textASCII)
#replace sequences of "-" with whitespace
textASCII<-gsub("-+"," ",textASCII)
#remove extra whitespace, quotes, spaces before commas
#textASCII <- scrubber(textASCII)
#strip digits and force all chars to lower case
textASCII <- strip(textASCII,char.keep="'",apostrophe.remove=FALSE)
#### Tokenize the Corpus
# for final test set reduce frequency threshold for keepers
v<-NGramTokenizer(textASCII, Weka_control(min = 4, max = 4,delimiters=" \r\n\t.,;:\"()?!"))
termFreqTableT<-as.data.frame(table(v),stringsAsFactors=FALSE)
names(termFreqTableT)<-c("term","freq")
termFreqTableT<-termFreqTableT[termFreqTableT$freq>4,]
termFreqTableT<-termFreqTableT[order(-termFreqTableT$freq),]
l<-str_extract_all(termFreqTableT$term,"[^[:blank:]]+")
temp<-as.data.frame(do.call(rbind,l),stringsAsFactors = FALSE)
names(temp)<-c("s1","s2","s3","s4")
termFreqTableT<-cbind(termFreqTableT,temp)
remove(temp)
# write final test set to file
write.csv(termFreqTableT,paste("termFreqTableT",sizeLabel,".csv",sep=""), row.names = FALSE)
source('~/Projects/Capstone/textprediction/text-predict_v4.R', echo=TRUE)
View(testResults)
termFreqTable4[grep("^are you waiting",termFreqTable4$term),]
termFreqTable3[grep("^you waiting",termFreqTable3$term),]
termFreqTable2[grep("^waiting",termFreqTable2$term),]
quadFault[469]
source('~/Projects/Capstone/textprediction/text-predict_v4.R', echo=TRUE)
predResults[469,]
testResults[469,]
source('~/Projects/Capstone/textprediction/text-predict_v4.R', echo=TRUE)
news <- readLines("../data/en_US.news.txt",n=-1,encoding="ISO-8859-1")
blogs<- readLines("../data/en_US.blogs.txt",n=-1,encoding="ISO-8859-1")
twitter<- readLines("../data/en_US.twitter.txt",n=-1,encoding="ISO-8859-1")
text<-c(news,blogs,twitter)
#### sammple different set of elements for testing ####
#### sammple x elements to reduce  size of data set ####
set.seed(39900)
#red<-seq(1,length(Text),by=10)
text<-sample(Text,50000)
#### clean imported text and create corpus  #####
# find non-ascii chars and remove them
text_<-iconv(text, "ISO-8859-1", "ASCII", sub="byte")
textASCII<-gsub("<[0-9|a-z][0-9|a-z]>","",text_)
#test
#grep("<[0-9|a-z][0-9|a-z]>",text_)
#grep("<[0-9|a-z][0-9|a-z]>",textASCII)
# make some space
remove(news,blogs,twitter,text,text_)
# remove quotes and forward slashes (not covered by removePunctuation)
textASCII<-gsub("\"","",textASCII)
textASCII<-gsub("/","",textASCII)
#more tests
#grep("\"",textASCII)
#grep("/",textASCII)
#replace sequences of "-" with whitespace
textASCII<-gsub("-+"," ",textASCII)
#remove extra whitespace, quotes, spaces before commas
#textASCII <- scrubber(textASCII)
#strip digits and force all chars to lower case
textASCII <- strip(textASCII,char.keep="'",apostrophe.remove=FALSE)
#### Tokenize the Corpus
# for final test set reduce frequency threshold for keepers
v<-NGramTokenizer(textASCII, Weka_control(min = 4, max = 4,delimiters=" \r\n\t.,;:\"()?!"))
termFreqTableT<-as.data.frame(table(v),stringsAsFactors=FALSE)
names(termFreqTableT)<-c("term","freq")
termFreqTableT<-termFreqTableT[termFreqTableT$freq>9,]
termFreqTableT<-termFreqTableT[order(-termFreqTableT$freq),]
l<-str_extract_all(termFreqTableT$term,"[^[:blank:]]+")
temp<-as.data.frame(do.call(rbind,l),stringsAsFactors = FALSE)
names(temp)<-c("s1","s2","s3","s4")
termFreqTableT<-cbind(termFreqTableT,temp)
remove(temp)
# write final test set to file
write.csv(termFreqTableT,paste("termFreqTableT",sizeLabel,".csv",sep=""), row.names = FALSE)
View(termFreqTableT)
head(Text)
Text[grep("classgoog",Text)]
textA[grep("classgoog",Text)]
textASCII[grep("classgoog",textASCII)]
news <- readLines("../data/en_US.news.txt",n=-1,encoding="ISO-8859-1")
blogs<- readLines("../data/en_US.blogs.txt",n=-1,encoding="ISO-8859-1")
twitter<- readLines("../data/en_US.twitter.txt",n=-1,encoding="ISO-8859-1")
text<-c(news,blogs,twitter)
#### sammple different set of elements for testing ####
#### sammple x elements to reduce  size of data set ####
set.seed(39900)
#red<-seq(1,length(Text),by=10)
text<-sample(Text,50000)
head(text)
text_<-iconv(text, "ISO-8859-1", "ASCII", sub="byte")
head(text)
textASCII<-gsub("<[0-9|a-z][0-9|a-z]>","",text_)
head(text)
remove(news,blogs,twitter,text,text_)
head(textASCII)
textASCII<-gsub("\"","",textASCII)
head(textASCII)
textASCII<-gsub("/","",textASCII)
head(textASCII)
textASCII<-gsub("-+"," ",textASCII)
head(textASCII)
textASCII <- strip(textASCII,char.keep="'",apostrophe.remove=FALSE)
head(textASCII)
v<-NGramTokenizer(textASCII, Weka_control(min = 4, max = 4,delimiters=" \r\n\t.,;:\"()?!"))
head(textASCII)
head(v)
textASCII[grep("classgoog",textASCII)]
news <- readLines("../data/en_US.news.txt",n=-1,encoding="ISO-8859-1")
blogs<- readLines("../data/en_US.blogs.txt",n=-1,encoding="ISO-8859-1")
twitter<- readLines("../data/en_US.twitter.txt",n=-1,encoding="ISO-8859-1")
Text<-c(news,blogs,twitter)
#### sammple different set of elements for testing ####
#### sammple x elements to reduce  size of data set ####
set.seed(39900)
#red<-seq(1,length(Text),by=10)
text<-sample(Text,50000)
Text[grep("classgoog",Text)]
text[grep("classgoog",text)]
text_<-iconv(text, "ISO-8859-1", "ASCII", sub="byte")
textASCII<-gsub("<[0-9|a-z][0-9|a-z]>","",text_)
#test
#grep("<[0-9|a-z][0-9|a-z]>",text_)
#grep("<[0-9|a-z][0-9|a-z]>",textASCII)
# make some space
remove(news,blogs,twitter,text,text_)
# remove quotes and forward slashes (not covered by removePunctuation)
textASCII<-gsub("\"","",textASCII)
textASCII<-gsub("/","",textASCII)
#more tests
#grep("\"",textASCII)
#grep("/",textASCII)
#replace sequences of "-" with whitespace
textASCII<-gsub("-+"," ",textASCII)
#remove extra whitespace, quotes, spaces before commas
#textASCII <- scrubber(textASCII)
#strip digits and force all chars to lower case
textASCII <- strip(textASCII,char.keep="'",apostrophe.remove=FALSE)
#### Tokenize the Corpus
# for final test set reduce frequency threshold for keepers
v<-NGramTokenizer(textASCII, Weka_control(min = 4, max = 4,delimiters=" \r\n\t.,;:\"()?!"))
textASCII[grep("classgoog",textASCII)]
which(textASCII[grep("classgoog",textASCII)])
which(grep("classgoog",textASCII))
which(grepl("classgoog",textASCII))
Text[3043]
text[3043]
textA[3043]
textASCII[3043]
news <- readLines("../data/en_US.news.txt",n=-1,encoding="ISO-8859-1")
blogs<- readLines("../data/en_US.blogs.txt",n=-1,encoding="ISO-8859-1")
twitter<- readLines("../data/en_US.twitter.txt",n=-1,encoding="ISO-8859-1")
Text<-c(news,blogs,twitter)
#### sammple different set of elements for testing ####
#### sammple x elements to reduce  size of data set ####
set.seed(39900)
#red<-seq(1,length(Text),by=10)
text<-sample(Text,50000)
#### clean imported text and create corpus  #####
# find non-ascii chars and remove them
text_<-iconv(text, "ISO-8859-1", "ASCII", sub="byte")
textASCII<-gsub("<[0-9|a-z][0-9|a-z]>","",text_)
textASCII[3043]
text[3043]
set.seed(39901)
#red<-seq(1,length(Text),by=10)
text<-sample(Text,50000)
text[3043]
text[grep("classgoog",text)]
text_<-iconv(text, "ISO-8859-1", "ASCII", sub="byte")
text[grep("classgoog",text)]
textASCII<-gsub("<[0-9|a-z][0-9|a-z]>","",text_)
textASCII[grep("classgoog",textASCII)]
remove(news,blogs,twitter,text,text_)
textASCII<-gsub("\"","",textASCII)
textASCII[grep("classgoog",textASCII)]
textASCII<-gsub("/","",textASCII)
textASCII[grep("classgoog",textASCII)]
textASCII<-gsub("-+"," ",textASCII)
textASCII[grep("classgoog",textASCII)]
textASCII <- strip(textASCII,char.keep="'",apostrophe.remove=FALSE)
textASCII[grep("classgoog",textASCII)]
v<-NGramTokenizer(textASCII, Weka_control(min = 4, max = 4,delimiters=" \r\n\t.,;:\"()?!"))
termFreqTableT<-as.data.frame(table(v),stringsAsFactors=FALSE)
names(termFreqTableT)<-c("term","freq")
termFreqTableT<-termFreqTableT[termFreqTableT$freq>9,]
termFreqTableT<-termFreqTableT[order(-termFreqTableT$freq),]
l<-str_extract_all(termFreqTableT$term,"[^[:blank:]]+")
temp<-as.data.frame(do.call(rbind,l),stringsAsFactors = FALSE)
names(temp)<-c("s1","s2","s3","s4")
termFreqTableT<-cbind(termFreqTableT,temp)
remove(temp)
# write final test set to file
write.csv(termFreqTableT,paste("termFreqTableT",sizeLabel,".csv",sep=""), row.names = FALSE)
source('~/Projects/Capstone/textprediction/text-predict_v4.R', echo=TRUE)
View(testResults)
quadFault[79]
termFreqTable4[grep("^I don't know",termFreqTable4$term,)]
termFreqTable4[grep("^i don't know",termFreqTable4$term,)]
termFreqTable4[grep("^i don't know",termFreqTable4$term),]
termFreqTable3[grep("^don't know",termFreqTable3$term),]
source('~/Projects/Capstone/textprediction/text-predict_v4.R', echo=TRUE)
shiny::runApp('shiny')
grep("[^[:blank:]]+$","this is not a love song")
str_extract("this is not a love song","[^[:blank:]]+$")
shiny::runApp('shiny')
!nrow(termFreqTable)
shiny::runApp('shiny')
i<-""
length(i)
i<-NULL
length(i)
shiny::runApp('shiny')
termFreqTable[5:6,]
shiny::runApp('shiny')
install.packages(shinyApps)
install.packages("shinyApps")
devtools::install_github("rstudio/shinyapps")
library(shinyapps)
runApp(quiet=TRUE)
getwd()
cd shiny
setwd("~/Projects/Capstone/textprediction/shiny")
runApp(quiet=TRUE)
runAPP
runApp
runApp()
shiny::runApp()
runApp(quiet=FALSE)
remove(shinyapps)
library(shiny)
